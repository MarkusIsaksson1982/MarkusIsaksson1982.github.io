<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title></title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
<pre style="white-space: pre-wrap; font-family: sans-serif;">
---
Type: Code artifact generated by Grok 4 
Author: Grok 4
File name(s): q_learning_extension.py
Keywords: [not added yet]
---

# q_learning_tictactoe_ai.py
# ARTI Level 3 Extension: Reinforcement Learning (Q-learning) for Tic-Tac-Toe.
# Builds on minimax: AI learns from experience instead of exhaustive search.
# Aligns with ARTI: Comparing rule-based vs. ML approaches, ethics in training data.
# Students: Train the AI, tweak params, discuss if "learned" biases emerge (e.g., from random opponents).

import numpy as np
import random
import pickle  # For saving/loading Q-table

# Game constants (same as before)
BOARD_SIZE = 3
EMPTY = 0  # Use ints for easier hashing
PLAYER_X = 1  # Human
PLAYER_O = -1  # AI

class TicTacToeQL:
    def __init__(self):
        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=int)
        self.current_player = PLAYER_X

    def reset(self):
        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=int)
        self.current_player = PLAYER_X

    def get_state(self):
        """Hash board to string for Q-table key."""
        return str(self.board.reshape(BOARD_SIZE * BOARD_SIZE))

    def is_valid_move(self, action):
        row, col = divmod(action, BOARD_SIZE)
        return self.board[row, col] == EMPTY

    def make_move(self, action, player):
        if self.is_valid_move(action):
            row, col = divmod(action, BOARD_SIZE)
            self.board[row, col] = player
            return True
        return False

    def check_winner(self):
        # Rows, columns, diagonals (adapted to int board)
        for i in range(BOARD_SIZE):
            if np.all(self.board[i, :] == PLAYER_O) or np.all(self.board[:, i] == PLAYER_O):
                return PLAYER_O
        if np.all(np.diag(self.board) == PLAYER_O) or np.all(np.diag(np.fliplr(self.board)) == PLAYER_O):
            return PLAYER_O
        if np.all(self.board[i, :] == PLAYER_X) or np.all(self.board[:, i] == PLAYER_X):
            return PLAYER_X
        if np.all(np.diag(self.board) == PLAYER_X) or np.all(np.diag(np.fliplr(self.board)) == PLAYER_X):
            return PLAYER_X
        if np.all(self.board != EMPTY):
            return 0  # Draw
        return None

    def print_board(self):
        symbols = {EMPTY: ' ', PLAYER_X: 'X', PLAYER_O: 'O'}
        for row in self.board:
            print(' | '.join(symbols[cell] for cell in row))
            print('-' * 5)
        print()

class QLearningAgent:
    def __init__(self, epsilon=0.1, alpha=0.1, gamma=0.9):
        self.q_table = {}  # State -> action values
        self.epsilon = epsilon  # Exploration rate (imperfection)
        self.alpha = alpha  # Learning rate
        self.gamma = gamma  # Discount factor
        self.actions = list(range(BOARD_SIZE * BOARD_SIZE))  # 0-8

    def get_q(self, state, action):
        if state not in self.q_table:
            self.q_table[state] = np.zeros(len(self.actions))
        return self.q_table[state][action]

    def choose_action(self, state, valid_moves):
        if random.random() < self.epsilon:
            return random.choice(valid_moves)  # Explore (imperfect)
        else:
            q_values = [self.get_q(state, a) if a in valid_moves else -np.inf for a in self.actions]
            return np.argmax(q_values)  # Exploit best

    def update_q(self, state, action, reward, next_state):
        best_next = max([self.get_q(next_state, a) for a in self.actions])
        self.q_table[state][action] += self.alpha * (reward + self.gamma * best_next - self.q_table[state][action])

    def save_q_table(self, filename='q_table.pkl'):
        with open(filename, 'wb') as f:
            pickle.dump(self.q_table, f)

    def load_q_table(self, filename='q_table.pkl'):
        try:
            with open(filename, 'rb') as f:
                self.q_table = pickle.load(f)
        except FileNotFoundError:
            pass

def train_agent(episodes=10000):
    game = TicTacToeQL()
    agent = QLearningAgent()
    for episode in range(episodes):
        game.reset()
        state = game.get_state()
        done = False
        while not done:
            if game.current_player == PLAYER_O:  # AI turn
                valid_moves = [a for a in agent.actions if game.is_valid_move(a)]
                action = agent.choose_action(state, valid_moves)
                game.make_move(action, PLAYER_O)
                next_state = game.get_state()
                winner = game.check_winner()
                if winner == PLAYER_O:
                    reward = 1
                    done = True
                elif winner == PLAYER_X:
                    reward = -1
                    done = True
                elif winner == 0:
                    reward = 0
                    done = True
                else:
                    reward = 0
                agent.update_q(state, action, reward, next_state)
                state = next_state
                game.current_player = PLAYER_X
            else:  # Random opponent
                valid_moves = [a for a in agent.actions if game.is_valid_move(a)]
                if valid_moves:
                    action = random.choice(valid_moves)
                    game.make_move(action, PLAYER_X)
                    state = game.get_state()  # Update state after opponent
                    winner = game.check_winner()
                    if winner:  # Reward AI based on outcome
                        done = True
                        reward = -1 if winner == PLAYER_X else (1 if winner == PLAYER_O else 0)
                        # Backpropagate to last AI action (simplified)
                game.current_player = PLAYER_O
        if episode % 1000 == 0:
            print(f"Episode {episode} completed.")
    agent.save_q_table()
    return agent

def play_game(agent):
    game = TicTacToeQL()
    agent.load_q_table()
    print("Welcome to Q-Learning Tic-Tac-Toe! You are X.")
    while True:
        game.print_board()
        winner = game.check_winner()
        if winner is not None:
            if winner == 0:
                print("Draw!")
            else:
                print(f"{ 'AI' if winner == PLAYER_O else 'You' } wins!")
            break
        if game.current_player == PLAYER_X:
            action = int(input("Your move (0-8, row-major): "))
            if not game.make_move(action, PLAYER_X):
                print("Invalid. Try again.")
                continue
        else:
            state = game.get_state()
            valid_moves = [a for a in agent.actions if game.is_valid_move(a)]
            action = agent.choose_action(state, valid_moves)
            game.make_move(action, PLAYER_O)
            print(f"AI moves to {action}")
        game.current_player = -game.current_player  # Switch

# Educational Usage:
# 1. Train: agent = train_agent(10000)
# 2. Play: play_game(agent)
# Discussion: How does training episodes affect performance? Ethics: If trained on biased opponents, does AI inherit flaws?

if __name__ == "__main__":
    # Example: Train and play
    agent = train_agent(5000)  # Reduce for quick demo
    play_game(agent)
</pre>  
</body>
</html>
