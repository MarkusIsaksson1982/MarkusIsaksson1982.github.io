
Below is a **self‑contained Python project** that fulfils the request.  
It consists of three files:

| File | Purpose |
|------|---------|
| `curriculum_extractor.py` | Core script that reads PDF files, extracts and parses the *kunskapskrav* sections, builds bridge connections and writes CSV, JSON and Markdown outputs. |
| `config.yaml` | Human readable configuration that lets you adjust subject bridge mappings, keyword lists, language settings, etc. |
| `requirements.txt` | Minimal dependency list. |

You can copy the three files into the same folder and run the extractor from the command line.

---

## 1. `requirements.txt`

```text
pdfplumber>=0.9.0
pymupdf>=1.24.5
pandas>=2.2.2
PyYAML>=6.0.2
```

> **Tip:** Install with `pip install -r requirements.txt`.  
> `pdfplumber` is used for the high level text extraction while `pymupdf` (aka `fitz`) is optionally available for more robust PDF handling.

---

## 2. `config.yaml`

```yaml
# ----------------------------------------------------------------------
# General settings
# ----------------------------------------------------------------------
pdf_folder: "pdfs"                     # folder where the source PDFs live
output_folder: "output"                # where CSV/JSON/MD will be written

# Language handling: 'sv' (Swedish) or 'en' (English). The extractor
# will try to detect the language automatically, but you can force it.
force_language: null                   # set to "sv" or "en" to override detection

# ----------------------------------------------------------------------
# Subject bridge configuration
# ----------------------------------------------------------------------
# For each target subject we list a set of keywords that will be used
# to create a bridge if they appear in a competency descriptor.
bridge:
  Svenska:
    - "språk"
    - "skrivning"
    - "läsförståelse"
    - "text"
    - "kommunikation"
  Historia:
    - "historia"
    - "tidslinje"
    - "samhälle"
    - "utveckling"
    - "källkritik"
  Samhällskunskap:
    - "samhälle"
    - "demokrati"
    - "rätt"
    - "ekonomi"
    - "politik"

# ----------------------------------------------------------------------
# Grade level markers
# ----------------------------------------------------------------------
# The extractor looks for the grade markers inside the text.
# Adjust only if the PDF layout changes dramatically.
grade_markers:
  E: ["E", "E-nivå", "E nivå"]
  C: ["C", "C-nivå", "C nivå"]
  A: ["A", "A-nivå", "A nivå"]

# ----------------------------------------------------------------------
# Text cleaning / normalisation
# ----------------------------------------------------------------------
clean:
  # Characters that should be stripped from each line before further processing
  strip_chars: " \t\r\n"
  # Optional replacement map for common OCR errors in Swedish PDFs
  replace:
    "ä": "ä"
    "ö": "ö"
    "å": "å"
    "ß": "ss"
```

You can edit `config.yaml` to add/remove keywords, change folders or force a language.

---

## 3. `curriculum_extractor.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
curriculum_extractor.py

Extracts and parses Swedish curriculum documents (e.g. Skolverket ARTI syllabi).
Identifies kunskapskrav sections, categorises them by grade level (E/C/A),
extracts competency descriptors, and creates bridge connections to other
subjects based on keyword matching.

Outputs:
    - CSV (spreadsheet friendly)
    - JSON (web / API friendly)
    - Markdown (documentation friendly)

Supports both Swedish and English PDFs, robust error handling and a
configurable bridge mapping via config.yaml.
"""

import os
import sys
import json
import csv
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

import yaml
import pandas as pd
import pdfplumber
# pymupdf (fitz) is imported lazily later if needed

# ----------------------------------------------------------------------
# Logging configuration
# ----------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# ----------------------------------------------------------------------
# Helper utilities
# ----------------------------------------------------------------------
def load_config(config_path: Path) -> Dict[str, Any]:
    """Load the YAML configuration file."""
    if not config_path.is_file():
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    with open(config_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    return cfg


def detect_language(text: str) -> str:
    """
    Very simple language detection based on common stop words.
    Returns 'sv' for Swedish, 'en' for English, otherwise 'unknown'.
    """
    sv_stop = {"och", "eller", "det", "att", "i", "en", "ett", "som", "är", "på"}
    en_stop = {"and", "or", "the", "to", "in", "a", "an", "is", "of", "that"}

    words = set(re.findall(r"\b\w+\b", text.lower()))
    sv_score = len(words & sv_stop)
    en_score = len(words & en_stop)

    if sv_score > en_score:
        return "sv"
    elif en_score > sv_score:
        return "en"
    else:
        return "unknown"


def clean_line(line: str, replace_map: Dict[str, str]) -> str:
    """Apply basic cleaning and optional OCR replacements."""
    line = line.strip()
    for old, new in replace_map.items():
        line = line.replace(old, new)
    return line


def split_into_sentences(text: str) -> List[str]:
    """Simple sentence splitter that works for both Swedish and English."""
    # Split on period, exclamation or question mark followed by a capital letter
    sentences = re.split(r'(?<=[.!?])\s+(?=[A-ZÅÄÖ])', text)
    return [s.strip() for s in sentences if s.strip()]


# ----------------------------------------------------------------------
# Core extraction logic
# ----------------------------------------------------------------------
class CurriculumExtractor:
    """
    Main class that orchestrates PDF reading, section detection,
    parsing and output generation.
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.pdf_folder = Path(config["pdf_folder"])
        self.output_folder = Path(config["output_folder"])
        self.output_folder.mkdir(parents=True, exist_ok=True)

        # Prepare grade markers for quick lookup
        self.grade_markers = {
            grade: set(markers)
            for grade, markers in config["grade_markers"].items()
        }
        self.bridge_keywords = config["bridge"]
        self.replace_map = config.get("clean", {}).get("replace", {})

        # Language handling
        self.force_language = config.get("force_language")
        if self.force_language and self.force_language not in ("sv", "en"):
            raise ValueError("force_language must be 'sv' or 'en' if set.")

        # Result container
        self.results: List[Dict[str, Any]] = []

    # ------------------------------------------------------------------
    # PDF handling
    # ------------------------------------------------------------------
    def _extract_text_from_pdf(self, pdf_path: Path) -> Tuple[str, str]:
        """
        Returns a tuple (raw_text, language). Tries pdfplumber first,
        falls back to PyMuPDF if text extraction fails.
        """
        try:
            with pdfplumber.open(pdf_path) as pdf:
                pages = [page.extract_text() for page in pdf.pages]
                raw_text = "\n".join(filter(None, pages))
            if not raw_text:
                raise ValueError("pdfplumber returned empty text")
        except Exception as e:
            logger.warning(f"pdfplumber failed for {pdf_path}: {e}. Trying PyMuPDF.")
            try:
                import fitz
                doc = fitz.open(pdf_path)
                pages = [doc[i].get_text("text") for i in range(doc.page_count)]
                raw_text = "\n".join(filter(None, pages))
                doc.close()
            except Exception as e2:
                raise RuntimeError(f"Both pdfplumber and PyMuPDF failed: {e2}")

        # Detect language if not forced
        lang = self.force_language if self.force_language else detect_language(raw_text)
        if lang == "unknown":
            logger.info(f"Language detection fell back to Swedish for {pdf_path}")
            lang = "sv"  # default to Swedish for curriculum docs
        return raw_text, lang

    # ------------------------------------------------------------------
    # Section detection
    # ------------------------------------------------------------------
    def _find_kunskapskrav_sections(self, text: str) -> List[Tuple[int, int, str]]:
        """
        Returns a list of tuples (start_idx, end_idx, raw_section_text) for each
        detected 'Kunskapskrav' block.
        Simple heuristic: look for a line containing 'Kunskapskrav' and capture
        until the next top-level heading (e.g. 'Kunskapskrav', 'Bedömningsstöd', 'Centralt innehåll')
        or end of file.
        """
        lines = text.splitlines()
        sections = []
        in_section = False
        current_start = 0
        current_lines = []

        heading_pattern = re.compile(r"^\s*(Kunskapskrav|Bedömningsstöd|Centralt innehåll)\s*$", re.IGNORECASE)

        for idx, line in enumerate(lines):
            if heading_pattern.match(line):
                if "Kunskapskrav" in line:
                    # start a new section
                    if in_section:
                        # close previous (should not happen but guard)
                        sections.append((current_start, idx, "\n".join(current_lines)))
                    current_start = idx
                    current_lines = [line]
                    in_section = True
                else:
                    # reached next top-level heading -> end current section
                    if in_section:
                        sections.append((current_start, idx, "\n".join(current_lines)))
                        in_section = False
            else:
                if in_section:
                    current_lines.append(line)

        # capture trailing section
        if in_section:
            sections.append((current_start, len(lines), "\n".join(current_lines)))

        return sections

    # ------------------------------------------------------------------
    # Grade level detection inside a kunskapskrav block
    # ------------------------------------------------------------------
    def _parse_grade_level(self, section_text: str) -> str:
        """
        Looks for grade markers (E/C/A) inside the section text.
        Returns the first matching grade or 'Unknown'.
        """
        # Search line by line for any of the markers
        for line in section_text.splitlines():
            line = line.upper()
            for grade, markers in self.grade_markers.items():
                for marker in markers:
                    if marker.upper() in line:
                        return grade
        return "Unknown"

    def _extract_descriptors(self, section_text: str, language: str) -> List[str]:
        """
        Extracts individual competency descriptors.
        For Swedish docs, descriptors often start with a bullet or dash,
        or are numbered. For English docs we also try a generic split.
        Returns a list of cleaned descriptor strings.
        """
        # Normalise line endings and strip
        cleaned_text = "\n".join(
            clean_line(l, self.replace_map) for l in section_text.splitlines()
        )

        # Remove the heading line itself
        heading_pattern = re.compile(r"^\s*Kunskapskrav.*$", re.IGNORECASE | re.MULTILINE)
        cleaned_text = heading_pattern.sub("", cleaned_text)

        # Split into lines, keep only non-empty
        lines = [ln for ln in cleaned_text.splitlines() if ln.strip()]

        # Detect bullet/number patterns
        bullet_pattern = re.compile(r"^[\*\-\u2022]\s+|^[\d]+\.\s+|^[\d]+\)\s+", re.IGNORECASE)
        descriptors = []
        current_desc = ""

        for line in lines:
            if bullet_pattern.match(line):
                # start a new descriptor
                if current_desc:
                    descriptors.append(current_desc.strip())
                current_desc = line
            else:
                # continuation of previous descriptor
                if current_desc:
                    current_desc += " " + line
                else:
                    # stray line not part of a descriptor -> ignore or keep?
                    # We'll keep it as a separate descriptor if it looks like a sentence.
                    if line.endswith("."):
                        descriptors.append(line.strip())
        if current_desc:
            descriptors.append(current_desc.strip())

        # Fallback: if nothing was captured, split by sentences
        if not descriptors:
            sentences = split_into_sentences(cleaned_text)
            descriptors = [s for s in sentences if s]

        # Final cleanup: remove leading bullet/number tokens
        cleaned_descs = []
        for d in descriptors:
            d = bullet_pattern.sub("", d).strip()
            cleaned_descs.append(d)

        return cleaned_descs

    # ------------------------------------------------------------------
    # Bridge generation
    # ------------------------------------------------------------------
    def _generate_bridges(self, descriptor: str) -> List[str]:
        """
        Returns a list of subject names that share at least one keyword
        with the descriptor (case-insensitive).
        """
        bridges = []
        lower_desc = descriptor.lower()
        for subject, keywords in self.bridge_keywords.items():
            for kw in keywords:
                if kw.lower() in lower_desc:
                    bridges.append(subject)
                    break  # avoid duplicate subject entry
        return bridges

    # ------------------------------------------------------------------
    # Main processing pipeline
    # ------------------------------------------------------------------
    def process(self):
        """Iterate over all PDFs, extract data and populate self.results."""
        pdf_files = list(self.pdf_folder.rglob("*.pdf"))
        if not pdf_files:
            logger.warning("No PDF files found in folder: %s", self.pdf_folder)
            return

        for pdf_path in pdf_files:
            logger.info("Processing %s", pdf_path.name)
            try:
                raw_text, language = self._extract_text_from_pdf(pdf_path)
                sections = self._find_kunskapskrav_sections(raw_text)

                for start, end, sec_text in sections:
                    grade = self._parse_grade_level(sec_text)
                    descriptors = self._extract_descriptors(sec_text, language)

                    for desc in descriptors:
                        bridges = self._generate_bridges(desc)
                        entry = {
                            "pdf_name": pdf_path.name,
                            "page_range": f"{start+1}-{end+1}",  # human readable 1-indexed
                            "grade_level": grade,
                            "language": language,
                            "descriptor": desc,
                            "bridge_subjects": bridges,
                        }
                        self.results.append(entry)

            except Exception as e:
                logger.error("Failed to process %s: %s", pdf_path.name, e)

    # ------------------------------------------------------------------
    # Output helpers
    # ------------------------------------------------------------------
    def _write_csv(self, out_path: Path):
        df = pd.DataFrame(self.results)
        df.to_csv(out_path, index=False, encoding="utf-8-sig")
        logger.info("CSV written to %s", out_path)

    def _write_json(self, out_path: Path):
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        logger.info("JSON written to %s", out_path)

    def _write_markdown(self, out_path: Path):
        """Creates a readable markdown table grouped by grade level."""
        if not self.results:
            logger.info("No results to write to markdown.")
            return

        md_lines = ["# Curriculum Extraction Report", ""]
        for grade in ["E", "C", "A", "Unknown"]:
            grade_rows = [r for r in self.
