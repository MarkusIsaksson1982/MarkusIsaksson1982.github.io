# Swedish curriculum parser (PyMuPDF / pdfplumber)

Below is a single Python script that extracts and parses Swedish curriculum PDFs (e.g., Skolverket ARTI syllabi).
Features:

* Supports PyMuPDF (`fitz`) **and** `pdfplumber` for text extraction (tries fitz first, falls back to pdfplumber).
* Identifies **Kunskapskrav** sections and categorizes items by grade/level (E / C / A).
* Extracts short competency descriptors (sentence-level).
* Generates **bridge connections** to other subjects (Svenska, Historia, Samhällskunskap by default) using configurable keyword mappings (YAML/JSON config).
* Outputs structured data: **CSV**, **JSON**, and **Markdown**.
* Handles malformed PDFs with graceful fallbacks and logging.
* Supports both Swedish and English text extraction; config can include language-specific keywords.
* CLI-friendly: pass input PDF(s) and config file; batch mode supported.

---

## Install required packages

```bash
pip install pymupdf pdfplumber pandas pyyaml regex langdetect
```

(`langdetect` is optional but included to help language-aware processing; errors are handled if not present.)

---

## `curriculum_parser.py`

```python
#!/usr/bin/env python3
"""
curriculum_parser.py

Extracts and parses Swedish curriculum PDFs (Kunskapskrav sections),
categorizes by grade levels (E/C/A), extracts competency descriptors,
and proposes cross-subject bridges based on keyword matching.

Outputs: JSON, CSV, and Markdown.

Usage:
    python curriculum_parser.py input.pdf --config config.yml --outdir output_folder
    python curriculum_parser.py *.pdf --config config.yml
"""

from __future__ import annotations
import sys
import os
import re
import json
import csv
import argparse
import logging
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, Tuple
import pathlib

# Optional imports
try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None

try:
    import pdfplumber
except Exception:
    pdfplumber = None

try:
    import pandas as pd
except Exception:
    pd = None

try:
    import yaml
except Exception:
    yaml = None

# Optional language detection
try:
    from langdetect import detect
except Exception:
    detect = None

# Logging
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")


DEFAULT_CONFIG = {
    "subjects": {
        "Svenska": ["text", "skriva", "läs", "tala", "språk", "läsa", "skrivförmåga", "analysera"],
        "Historia": ["källa", "tidslinje", "histor", "historisk", "kronologi", "period"],
        "Samhällskunskap": ["politik", "samhäll", "demokrati", "rätt", "stat", "ekonomi", "medborg"],
    },
    "grade_markers": ["E", "C", "A"],  # order matters for priority if needed
    # regex patterns for locating 'kunskapskrav' sections (swedish & english)
    "section_headings": [r"kunskapskrav", r"knowledge requirements", r"assessment criteria"],
    # minimal sentence length to consider as descriptor
    "min_descriptor_words": 3,
    # how many top bridges to list
    "max_bridges": 3
}


@dataclass
class Competency:
    grade: str
    text: str
    descriptors: List[str]
    bridges: Dict[str, int]  # subject -> score


@dataclass
class ParsedDocument:
    source_file: str
    language: Optional[str]
    raw_text: str
    competencies: List[Competency]


#######################
# Text extraction
#######################

def extract_text_pymupdf(path: str) -> str:
    if fitz is None:
        raise RuntimeError("PyMuPDF (fitz) not installed.")
    try:
        doc = fitz.open(path)
    except Exception as e:
        raise RuntimeError(f"PyMuPDF failed to open file: {e}")
    texts = []
    for page in doc:
        try:
            texts.append(page.get_text("text"))
        except Exception:
            # some pages can fail; skip with warning
            logging.warning(f"PyMuPDF: failed to extract page {page.number} from {path}")
    return "\n".join(texts)


def extract_text_pdfplumber(path: str) -> str:
    if pdfplumber is None:
        raise RuntimeError("pdfplumber not installed.")
    texts = []
    try:
        with pdfplumber.open(path) as pdf:
            for i, p in enumerate(pdf.pages):
                try:
                    texts.append(p.extract_text() or "")
                except Exception:
                    logging.warning(f"pdfplumber: failed to extract page {i} from {path}")
    except Exception as e:
        raise RuntimeError(f"pdfplumber failed to open file: {e}")
    return "\n".join(texts)


def extract_text(path: str) -> Tuple[str, str]:
    """Try PyMuPDF first, then pdfplumber; return (text, engine_name)."""
    errors = []
    if fitz is not None:
        try:
            t = extract_text_pymupdf(path)
            logging.info(f"Extracted text with PyMuPDF from {path}")
            return t, "pymupdf"
        except Exception as e:
            errors.append(("pymupdf", str(e)))
            logging.debug(f"PyMuPDF error: {e}")
    if pdfplumber is not None:
        try:
            t = extract_text_pdfplumber(path)
            logging.info(f"Extracted text with pdfplumber from {path}")
            return t, "pdfplumber"
        except Exception as e:
            errors.append(("pdfplumber", str(e)))
            logging.debug(f"pdfplumber error: {e}")
    # If both disabled or both failed:
    raise RuntimeError(f"No PDF extractor available or all extractors failed. Details: {errors}")


#######################
# Parsing logic
#######################

def detect_language(text: str) -> Optional[str]:
    if detect is None:
        return None
    try:
        lang = detect(text[:2000])
        return lang
    except Exception:
        return None


def split_into_sentences(text: str) -> List[str]:
    # Simple sentence splitter that handles Swedish/English punctuation.
    # We avoid heavy NLP libs for portability.
    # Split on ., ?, ! or newline if long lines
    sentences = []
    # Normalize newlines
    text = text.replace("\r", "\n")
    # First split by lines to preserve layout-based sentences
    for line in text.split("\n"):
        line = line.strip()
        if not line:
            continue
        # split by punctuation
        parts = re.split(r'(?<=[\.\?\!])\s+', line)
        for p in parts:
            p = p.strip()
            if p:
                sentences.append(p)
    return sentences


def find_sections(text: str, config: dict) -> List[Tuple[int, int, str]]:
    """
    Find sections whose headings match 'kunskapskrav' patterns.
    Return list of (start_idx, end_idx, heading_text) as character offsets.
    Simple heuristic: find heading occurrences and capture until next heading or end.
    """
    headings = config.get("section_headings", DEFAULT_CONFIG["section_headings"])
    pattern = re.compile(r'^(?:' + "|".join(headings) + r')\b.*$', re.IGNORECASE | re.MULTILINE)
    matches = list(pattern.finditer(text))
    sections = []
    if not matches:
        # Try looser search for the word anywhere
        pattern_loose = re.compile(r'\b(?:' + "|".join(headings) + r')\b', re.IGNORECASE)
        matches = list(pattern_loose.finditer(text))
    if not matches:
        return []
    for i, m in enumerate(matches):
        start = m.start()
        heading_text = m.group(0)
        if i + 1 < len(matches):
            end = matches[i + 1].start()
        else:
            end = len(text)
        sections.append((start, end, heading_text))
    return sections


def extract_grade_blocks(section_text: str, config: dict) -> List[Tuple[str, str]]:
    """
    Within a kunskapskrav section, find blocks associated with grades E/C/A.
    Heuristic: look for lines that start with grade markers, or contain '(E)' etc.
    Returns list of (grade, block_text).
    """
    grade_markers = config.get("grade_markers", DEFAULT_CONFIG["grade_markers"])
    # Build regex to find markers as standalone tokens or at line start
    marker_pattern = r'^(?:' + "|".join([re.escape(g) for g in grade_markers]) + r')\b.*$'
    lines = section_text.splitlines()
    blocks = []
    current_grade = None
    current_lines = []
    for line in lines:
        stripped = line.strip()
        # Detect if this line indicates a new grade block:
        m = re.match(marker_pattern, stripped, flags=re.IGNORECASE)
        # Also accept patterns like "E-nivå", "Betydelsen för nivå E", "E (godkänd)"
        if m or re.search(r'\b(' + "|".join([re.escape(g) for g in grade_markers]) + r')\b', stripped):
            # If we were collecting previous block, save it
            # Try to detect which grade this line refers to
            grade_found = None
            for g in grade_markers:
                if re.search(r'\b' + re.escape(g) + r'\b', stripped, flags=re.IGNORECASE):
                    grade_found = g.upper()
                    break
            if current_grade and current_lines:
                blocks.append((current_grade, "\n".join(current_lines).strip()))
                current_lines = []
            current_grade = grade_found or stripped[:2].upper()
            # Keep the current line as part of block (maybe heading)
            current_lines.append(stripped)
        else:
            # Continuation of current block if any
            if current_grade:
                current_lines.append(line)
            else:
                # No grade found yet: sometimes text shows "Kunskapskrav för E, C och A"
                # So try to see if line mentions "E, C och A" -> create a synthetic block
                if re.search(r'\bE[, ]+C[, ]+A\b', stripped, re.IGNORECASE) or "E C A" in stripped:
                    # create a block for each grade? For now aggregate under 'ALL'
                    if current_lines:
                        blocks.append(("ALL", "\n".join(current_lines).strip()))
                        current_lines = []
                    current_grade = "ALL"
                    current_lines.append(stripped)
                else:
                    # skip or collect as general preamble
                    current_lines.append(line)
    if current_grade and current_lines:
        blocks.append((current_grade, "\n".join(current_lines).strip()))
    # Post-process: merge consecutive "ALL" into others if needed
    return blocks


def extract_descriptors(block_text: str, config: dict) -> List[str]:
    """
    From a grade block, extract short competency descriptors (sentence-like).
    A simple approach: split into sentences and filter by length and keywords.
    """
    sentences = split_into_sentences(block_text)
    min_words = config.get("min_descriptor_words", DEFAULT_CONFIG["min_descriptor_words"])
    descriptors = []
    for s in sentences:
        words = [w for w in re.findall(r'\w+', s)]
        if len(words) >= min_words:
            descriptors.append(s.strip())
    # dedupe preserving order
    seen = set()
    dedup = []
    for d in descriptors:
        if d not in seen:
            dedup.append(d)
            seen.add(d)
    return dedup


def score_bridges(descriptor: str, subject_keywords: Dict[str, List[str]]) -> Dict[str, int]:
    """
    Score how well a descriptor maps to each subject via keyword matching.
    Returns subject->score (integer count).
    """
    scores = {}
    text = descriptor.lower()
    for subj, kws in subject_keywords.items():
        score = 0
        for kw in kws:
            # word boundary matching for accuracy
            if re.search(r'\b' + re.escape(kw.lower()) + r'\b', text):
                score += 1
        scores[subj] = score
    return scores


def propose_bridges(scores: Dict[str, int], max_bridges: int = 3) -> List[Tuple[str, int]]:
    """Return top N (subject, score) sorted by score desc, excluding zero scores."""
    items = [(s, sc) for s, sc in scores.items() if sc > 0]
    items.sort(key=lambda x: x[1], reverse=True)
    return items[:max_bridges]


#######################
# Main parsing per file
#######################

def parse_document(path: str, config: dict) -> ParsedDocument:
    # Extract
    try:
        text, engine = extract_text(path)
    except Exception as e:
        raise RuntimeError(f"Failed to extract text from {path}: {e}")

    # Basic normalization: replace multiple spaces
    text = re.sub(r'\u00A0', ' ', text)  # non-breakable space
    text = re.sub(r'\s+\n', '\n', text)
    # Detect language
    lang = detect_language(text) if detect else None

    # Find Kunskapskrav sections
    sections = find_sections(text, config)
    competencies: List[Competency] = []

    subject_keywords = config.get("subjects", DEFAULT_CONFIG["subjects"])
    max_bridges = config.get("max_bridges", DEFAULT_CONFIG["max_bridges"])

    if not sections:
        # fallback: try to search for "kunskapskrav" anywhere
        if re.search(r'kunskapskrav', text, flags=re.IGNORECASE):
            # use entire doc as a section
            sections = [(0, len(text), "kunskapskrav (fallback)")]
        else:
            logging.warning(f"No 'kunskapskrav' sections found in {path}")

    for start, end, heading in sections:
        section_text = text[start:end]
        grade_blocks = extract_grade_blocks(section_text, config)
        if not grade_blocks:
            # treat whole section as a single block
            grade_blocks = [("UNKNOWN", section_text)]
        for grade, block in grade_blocks:
            descriptors = extract_descriptors(block, config)
            # For each descriptor, compute bridges and aggregate for block
            block_bridges = {}
            for d in descriptors:
                sc = score_bridges(d, subject_keywords)
                for k, v in sc.items():
                    block_bridges[k] = block_bridges.get(k, 0) + v
            # Optionally propose bridges per descriptor
            bridges = {s: v for s, v in block_bridges.items() if v > 0}
            competencies.append(Competency(grade=grade, text=block, descriptors=descriptors, bridges=bridges))

    parsed = ParsedDocument(source_file=path, language=lang, raw_text=text, competencies=competencies)
    return parsed


#######################
# Output writers
#######################

def write_json(parsed: ParsedDocument, outpath: str):
    data = {
        "source_file": parsed.source_file,
        "language": parsed.language,
        "competencies": []
    }
    for c in parsed.competencies:
        data["competencies"].append({
            "grade": c.grade,
            "text": c.text,
            "descriptors": c.descriptors,
            "bridges": c.bridges
        })
    with open(outpath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    logging.info(f"Wrote JSON to {outpath}")


def write_csv(parsed: ParsedDocument, outpath: str):
    # Each row: source_file, grade, descriptor, bridges (as semicolon-separated subject:score)
    rows = []
    for c in parsed.competencies:
        # If there are descriptors, create one row per descriptor; otherwise create block-level row
        if c.descriptors:
            for desc in c.descriptors:
                bridges_repr = ";".join([f"{k}:{v}" for k, v in c.bridges.items()]) if c.bridges else ""
                rows.append([parsed.source_file, c.grade, desc, bridges_repr])
        else:
            bridges_repr = ";".join([f"{k}:{v}" for k, v in c.bridges.items()]) if c.bridges else ""
            rows.append([parsed.source_file, c.grade, c.text[:200], bridges_repr])
    # Write using csv module (avoids pandas dependency)
    with open(outpath, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["source_file", "grade", "descriptor", "bridges"])
        writer.writerows(rows)
    logging.info(f"Wrote CSV to {outpath}")


def write_markdown(parsed: ParsedDocument, outpath: str):
    lines = []
    lines.append(f"# Parsed curriculum: {os.path.basename(parsed.source_file)}")
    if parsed.language:
        lines.append(f"_Detected language: {parsed.language}_")
    lines.append("")
    for i, c in enumerate(parsed.competencies, start=1):
        lines.append(f"## Competency block {i} — Grade: {c.grade}")
        lines.append("")
        lines.append("**Extracted descriptors:**")
        if c.descriptors:
            for d in c.descriptors:
                lines.append(f"- {d}")
        else:
            lines.append("- (no compact descriptors found)")
        lines.append("")
        if c.bridges:
            lines.append("**Proposed subject bridges (subject:score):**")
            for subj, sc in sorted(c.bridges.items(), key=lambda x: x[1], reverse=True):
                lines.append(f"- {subj}: {sc}")
        else:
            lines.append("**Proposed subject bridges:** None")
        lines.append("")
    with open(outpath, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    logging.info(f"Wrote Markdown to {outpath}")


#######################
# Config handling
#######################

def load_config(path: Optional[str]) -> dict:
    if not path:
        return DEFAULT_CONFIG.copy()
    if not os.path.exists(path):
        logging.warning(f"Config file {path} not found — using defaults.")
        return DEFAULT_CONFIG.copy()
    if yaml and path.lower().endswith((".yml", ".yaml")):
        with open(path, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f) or {}
            merged = DEFAULT_CONFIG.copy()
            merged.update(cfg)
            return merged
    else:
        # try json
        try:
            with open(path, "r", encoding="utf-8") as f:
                cfg = json.load(f)
                merged = DEFAULT_CONFIG.copy()
                merged.update(cfg)
                return merged
        except Exception as e:
            logging.warning(f"Failed to parse config as JSON/YAML: {e}. Using defaults.")
            return DEFAULT_CONFIG.copy()


#######################
# CLI
#######################

def ensure_outdir(path: str):
    os.makedirs(path, exist_ok=True)


def main(argv=None):
    parser = argparse.ArgumentParser(description="Parse Swedish curriculum PDFs (kunskapskrav).")
    parser.add_argument("inputs", nargs="+", help="Input PDF file(s).")
    parser.add_argument("--config", help="Path to YAML/JSON config file for subject keywords and options.")
    parser.add_argument("--outdir", default="parsed_output", help="Output directory.")
    parser.add_argument("--format", choices=["all", "json", "csv", "md"], default="all", help="Which output formats to write.")
    parser.add_argument("--quiet", action="store_true")
    args = parser.parse_args(argv)

    if args.quiet:
        logging.getLogger().setLevel(logging.WARNING)

    cfg = load_config(args.config)
    ensure_outdir(args.outdir)

    for inp in args.inputs:
        path = inp
        try:
            parsed = parse_document(path, cfg)
        except Exception as e:
            logging.error(f"Failed to parse {path}: {e}")
            continue

        base = pathlib.Path(path).stem
        if args.format in ("all", "json"):
            write_json(parsed, os.path.join(args.outdir, f"{base}.json"))
        if args.format in ("all", "csv"):
            write_csv(parsed, os.path.join(args.outdir, f"{base}.csv"))
        if args.format in ("all", "md"):
            write_markdown(parsed, os.path.join(args.outdir, f"{base}.md"))

    logging.info("Done.")


if __name__ == "__main__":
    main()
```

---

## Example configuration (`config.yml`)

You can customize subject bridge keyword mappings and parsing behavior.

```yaml
subjects:
  Svenska:
    - text
    - skriva
    - läsa
    - tala
    - språk
    - läsförståelse
  Historia:
    - källa
    - historia
    - historisk
    - tidslinje
    - källkritik
  Samhällskunskap:
    - demokrati
    - politik
    - ekonomi
    - rätt
    - medborgare
grade_markers: ["E", "C", "A"]
section_headings:
  - "kunskapskrav"
  - "knowledge requirements"
min_descriptor_words: 3
max_bridges: 3
```

---

## Notes, limitations, and suggestions

* **Heuristics**: The script uses regex and simple sentence splitting; results depend on PDF text quality and formatting. Scanned PDFs without OCR will fail unless you run OCR beforehand (e.g., Tesseract).
* **Grade detection**: The script searches for grade markers (E/C/A). If documents use other conventions or more detailed headings (e.g., "Godkänd = E"), you can extend `grade_markers` in the config.
* **Language**: `langdetect` helps but is optional. You can create language-specific keyword lists in the config (e.g., English keywords for "knowledge requirements").
* **Improvements**:

  * Use spaCy or Nordict/NLTK for more accurate sentence segmentation and lemmatization (would improve keyword matching).
  * Use TF-IDF or word embeddings to suggest bridges beyond exact keyword matches.
  * Add a small UI or export to Google Sheets via API.
  * Add OCR fallback (e.g., use `pytesseract` + `pdf2image`) for image-only PDFs.
* **Error handling**: The script logs failures to extract certain pages and falls back between extraction engines. Malformed PDFs will be skipped with an explanatory log.

---

If you'd like, I can:

* Add OCR fallback for scanned PDFs.
* Provide an enhanced keyword mapping for **Svenska, Historia, Samhällskunskap** tuned to Skolverket vocabulary.
* Convert outputs directly to Google Sheets (requires OAuth credentials).
* Package this as a pip-installable tool with unit tests.

Which of those (if any) would you like next?
